import os
import re
import requests
import time
import hashlib
import random
import argparse
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urljoin
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

class SimpleImageCrawler:
    def __init__(self):
        self.visited_urls = set()
        self.downloaded = set()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
            'Referer': 'https://www.google.com/'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.max_retries = 2
        self.delay = (0.5, 1.5)

    def _get_hash(self, content):
        return hashlib.md5(content).hexdigest()

    def _find_next_page(self, current_url):
        # è‡ªåŠ¨è¯†åˆ«æ•°å­—åˆ†é¡µï¼ˆæ”¯æŒ page= / p= / è·¯å¾„æ•°å­—ï¼‰
        patterns = [
            r'(page|p)=(\d+)',          # åŒ¹é…æŸ¥è¯¢å‚æ•°
            r'/page/(\d+)',
            r'/(\d+)(/|\.html|$)'       # åŒ¹é…è·¯å¾„æœ«å°¾æ•°å­—
        ]
        
        for pattern in patterns:
            match = re.search(pattern, current_url)
            if match:
                if '=' in pattern:  # å¤„ç†å‚æ•°å½¢å¼
                    param, num = match.groups()
                    new_num = int(num) + 1
                    return current_url.replace(f"{param}={num}", f"{param}={new_num}")
                else:  # å¤„ç†è·¯å¾„å½¢å¼
                    num = match.group(1)
                    new_num = int(num) + 1
                    return current_url.replace(num, str(new_num), 1)
        return None

    def _fetch(self, url):
        options = Options()
        options.headless = True
        driver = webdriver.Chrome(options=options)
        driver.get(url)
        html = driver.page_source
        driver.quit()
        return html.encode('utf-8')

    def _extract_images(self, html, base_url):
        soup = BeautifulSoup(html, 'html.parser')
        urls = []
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src:
                clean_url = urljoin(base_url, src.split('?')[0])
                if clean_url.startswith('http'):
                    urls.append(clean_url)
        return list(set(urls))

    def _download(self, img_url):
        if img_url in self.downloaded:
            return

        try:
            resp = self.session.get(img_url, stream=True, timeout=10)
            resp.raise_for_status()
            
            content = resp.content
            content_hash = self._get_hash(content)
            
            if content_hash in self.downloaded:
                return
                
            # åˆ›å»ºä¿å­˜ç›®å½•
            os.makedirs('images', exist_ok=True)
            
            # ç”Ÿæˆå®‰å…¨æ–‡ä»¶å
            filename = re.sub(r'[^\w\-_.]', '', os.path.basename(img_url))[:100]
            save_path = os.path.join('images', filename)
            
            with open(save_path, 'wb') as f:
                f.write(content)
                
            self.downloaded.update([img_url, content_hash])
            print(f"âœ… å·²ä¿å­˜: {filename}")
            
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥ {img_url}: {str(e)}")

    def crawl(self, start_url, max_depth=5):
        current_url = start_url
        depth = 0
        
        while depth < max_depth:
            if current_url in self.visited_urls:
                break
                
            print(f"ğŸ” æ­£åœ¨çˆ¬å–: {current_url}")
            self.visited_urls.add(current_url)
            
            html = self._fetch(current_url)
            if not html:
                break
                
            # ä¸‹è½½å½“å‰é¡µå›¾ç‰‡
            img_urls = self._extract_images(html, current_url)
            with ThreadPoolExecutor(max_workers=6) as executor:
                executor.map(self._download, img_urls)
            
            # è·å–ä¸‹ä¸€é¡µ
            next_page = self._find_next_page(current_url)
            if not next_page:
                break
                
            current_url = next_page
            depth += 1

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--url', type=str, required=True, help='èµ·å§‹ç½‘å€')
    parser.add_argument('--depth', type=int, default=100, help='æœ€å¤§æ·±åº¦')
    args = parser.parse_args()
    
    crawler = SimpleImageCrawler()
    crawler.crawl(start_url=args.url, max_depth=args.depth)
